{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a59826bf-a5a5-4233-a8f9-b1c4db7216ee",
   "metadata": {},
   "source": [
    "# 17.2 Zadanie Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b8d9c-0992-4c10-8d19-b9a4e0ecce6b",
   "metadata": {},
   "source": [
    "#### Import niezbednych bibliotek i paczek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e36c2c-92e1-495a-8037-a78d9fd1df6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\piotr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\piotr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\piotr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import itertools\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70aed96-f2d7-406e-a8d8-22eb620eb096",
   "metadata": {},
   "source": [
    "#### Definicje wszystkich funkcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f16a14a-e809-4c3c-818f-130a283dc62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_puncation(text):\n",
    "    cleaned = ''.join([word for word in text if word not in string.punctuation])\n",
    "    return cleaned\n",
    "def tokenize(text):\n",
    "    # Usunięcie wielkich liter\n",
    "    clean_text = text.lower()\n",
    "    # Tokenizacja\n",
    "    tokenized_text = nltk.word_tokenize(clean_text)\n",
    "    return tokenized_text\n",
    "def remove_stopwords(text):\n",
    "    without_stopwords = [word for word in text if word not in stopwords]\n",
    "    return without_stopwords\n",
    "def stemming(text):\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    return stemmed_words\n",
    "def lemmatizing(text):\n",
    "    lemmatized_words = [lemmater.lemmatize(word) for word in text]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d49463-97b5-4112-8a80-573296373501",
   "metadata": {},
   "source": [
    "#### Import pliku z danymi i jego formatowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "169c218d-ef76-44bf-bed1-f22d19d6b27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piotr\\AppData\\Local\\Temp\\ipykernel_33660\\1996902043.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  spam_dataset['Spam'] = spam_dataset['Spam'].replace(['ham', 'spam'], [0, 1])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Spam                                               Text\n",
       "0        0  Go until jurong point, crazy.. Available only ...\n",
       "1        0                      Ok lar... Joking wif u oni...\n",
       "2        1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3        0  U dun say so early hor... U c already then say...\n",
       "4        0  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567     1  This is the 2nd time we have tried 2 contact u...\n",
       "5568     0              Will Ì_ b going to esplanade fr home?\n",
       "5569     0  Pity, * was in mood for that. So...any other s...\n",
       "5570     0  The guy did some bitching but I acted like i'd...\n",
       "5571     0                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_dataset = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\", usecols=[0, 1], names=['Spam', 'Text'],\n",
    "                           skiprows=1)\n",
    "spam_dataset['Spam'] = spam_dataset['Spam'].replace(['ham', 'spam'], [0, 1])\n",
    "spam_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05288c2-f9c6-4320-961c-1cb3eb919af2",
   "metadata": {},
   "source": [
    "#### Obróbka danych - czyszczenie, tokenizacja, usuwanie stop words, Stemming, Lametyzcja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f698ae3-42e2-4ccf-a164-d3ac44bc32e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam</th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "      <th>Tokenized_Text</th>\n",
       "      <th>WithoutStop_Text</th>\n",
       "      <th>Stemmed_Text</th>\n",
       "      <th>Lemmatized_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>Go until jurong point crazy Available only in ...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, gre...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, tho...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>[this, is, the, 2nd, time, we, have, tried, 2,...</td>\n",
       "      <td>[2nd, time, tried, 2, contact, u, u, å£750, po...</td>\n",
       "      <td>[2nd, time, tri, 2, contact, u, u, å£750, poun...</td>\n",
       "      <td>[2nd, time, tried, 2, contact, u, u, å£750, po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>Will Ì b going to esplanade fr home</td>\n",
       "      <td>[will, ì, b, going, to, esplanade, fr, home]</td>\n",
       "      <td>[ì, b, going, esplanade, fr, home]</td>\n",
       "      <td>[ì, b, go, esplanad, fr, home]</td>\n",
       "      <td>[ì, b, going, esplanade, fr, home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>Pity  was in mood for that Soany other suggest...</td>\n",
       "      <td>[pity, was, in, mood, for, that, soany, other,...</td>\n",
       "      <td>[pity, mood, soany, suggestions]</td>\n",
       "      <td>[piti, mood, soani, suggest]</td>\n",
       "      <td>[pity, mood, soany, suggestion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>The guy did some bitching but I acted like id ...</td>\n",
       "      <td>[the, guy, did, some, bitching, but, i, acted,...</td>\n",
       "      <td>[guy, bitching, acted, like, id, interested, b...</td>\n",
       "      <td>[guy, bitch, act, like, id, interest, buy, som...</td>\n",
       "      <td>[guy, bitching, acted, like, id, interested, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>Rofl Its true to its name</td>\n",
       "      <td>[rofl, its, true, to, its, name]</td>\n",
       "      <td>[rofl, true, name]</td>\n",
       "      <td>[rofl, true, name]</td>\n",
       "      <td>[rofl, true, name]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Spam                                               Text  \\\n",
       "0        0  Go until jurong point, crazy.. Available only ...   \n",
       "1        0                      Ok lar... Joking wif u oni...   \n",
       "2        1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        0  U dun say so early hor... U c already then say...   \n",
       "4        0  Nah I don't think he goes to usf, he lives aro...   \n",
       "...    ...                                                ...   \n",
       "5567     1  This is the 2nd time we have tried 2 contact u...   \n",
       "5568     0              Will Ì_ b going to esplanade fr home?   \n",
       "5569     0  Pity, * was in mood for that. So...any other s...   \n",
       "5570     0  The guy did some bitching but I acted like i'd...   \n",
       "5571     0                         Rofl. Its true to its name   \n",
       "\n",
       "                                           Cleaned_Text  \\\n",
       "0     Go until jurong point crazy Available only in ...   \n",
       "1                               Ok lar Joking wif u oni   \n",
       "2     Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3           U dun say so early hor U c already then say   \n",
       "4     Nah I dont think he goes to usf he lives aroun...   \n",
       "...                                                 ...   \n",
       "5567  This is the 2nd time we have tried 2 contact u...   \n",
       "5568                Will Ì b going to esplanade fr home   \n",
       "5569  Pity  was in mood for that Soany other suggest...   \n",
       "5570  The guy did some bitching but I acted like id ...   \n",
       "5571                          Rofl Its true to its name   \n",
       "\n",
       "                                         Tokenized_Text  \\\n",
       "0     [go, until, jurong, point, crazy, available, o...   \n",
       "1                        [ok, lar, joking, wif, u, oni]   \n",
       "2     [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3     [u, dun, say, so, early, hor, u, c, already, t...   \n",
       "4     [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "...                                                 ...   \n",
       "5567  [this, is, the, 2nd, time, we, have, tried, 2,...   \n",
       "5568       [will, ì, b, going, to, esplanade, fr, home]   \n",
       "5569  [pity, was, in, mood, for, that, soany, other,...   \n",
       "5570  [the, guy, did, some, bitching, but, i, acted,...   \n",
       "5571                   [rofl, its, true, to, its, name]   \n",
       "\n",
       "                                       WithoutStop_Text  \\\n",
       "0     [go, jurong, point, crazy, available, bugis, n...   \n",
       "1                        [ok, lar, joking, wif, u, oni]   \n",
       "2     [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3         [u, dun, say, early, hor, u, c, already, say]   \n",
       "4     [nah, dont, think, goes, usf, lives, around, t...   \n",
       "...                                                 ...   \n",
       "5567  [2nd, time, tried, 2, contact, u, u, å£750, po...   \n",
       "5568                 [ì, b, going, esplanade, fr, home]   \n",
       "5569                   [pity, mood, soany, suggestions]   \n",
       "5570  [guy, bitching, acted, like, id, interested, b...   \n",
       "5571                                 [rofl, true, name]   \n",
       "\n",
       "                                           Stemmed_Text  \\\n",
       "0     [go, jurong, point, crazi, avail, bugi, n, gre...   \n",
       "1                          [ok, lar, joke, wif, u, oni]   \n",
       "2     [free, entri, 2, wkli, comp, win, fa, cup, fin...   \n",
       "3         [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
       "4     [nah, dont, think, goe, usf, live, around, tho...   \n",
       "...                                                 ...   \n",
       "5567  [2nd, time, tri, 2, contact, u, u, å£750, poun...   \n",
       "5568                     [ì, b, go, esplanad, fr, home]   \n",
       "5569                       [piti, mood, soani, suggest]   \n",
       "5570  [guy, bitch, act, like, id, interest, buy, som...   \n",
       "5571                                 [rofl, true, name]   \n",
       "\n",
       "                                        Lemmatized_Text  \n",
       "0     [go, jurong, point, crazy, available, bugis, n...  \n",
       "1                        [ok, lar, joking, wif, u, oni]  \n",
       "2     [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "3         [u, dun, say, early, hor, u, c, already, say]  \n",
       "4     [nah, dont, think, go, usf, life, around, though]  \n",
       "...                                                 ...  \n",
       "5567  [2nd, time, tried, 2, contact, u, u, å£750, po...  \n",
       "5568                 [ì, b, going, esplanade, fr, home]  \n",
       "5569                    [pity, mood, soany, suggestion]  \n",
       "5570  [guy, bitching, acted, like, id, interested, b...  \n",
       "5571                                 [rofl, true, name]  \n",
       "\n",
       "[5572 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_dataset['Cleaned_Text'] = spam_dataset['Text'].apply(lambda x: remove_puncation(x))\n",
    "spam_dataset['Tokenized_Text'] = spam_dataset['Cleaned_Text'].apply(lambda x: tokenize(x))\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "spam_dataset['WithoutStop_Text'] = spam_dataset['Tokenized_Text'].apply(lambda x: remove_stopwords(x))\n",
    "stemmer = nltk.PorterStemmer()\n",
    "spam_dataset['Stemmed_Text'] = spam_dataset['WithoutStop_Text'].apply(lambda x: stemming(x))\n",
    "lemmater = nltk.WordNetLemmatizer()\n",
    "spam_dataset['Lemmatized_Text'] = spam_dataset['WithoutStop_Text'].apply(lambda x: lemmatizing(x))\n",
    "spam_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458d99e-b0de-437a-b42c-a6414ae9b14d",
   "metadata": {},
   "source": [
    "#### Rozdzielenie na X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c275ce57-6a22-493c-9fbb-f4ad6a90c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spam_dataset['Lemmatized_Text'].apply(lambda x: ' '.join(x))\n",
    "y = spam_dataset['Spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12280cfa-33f7-4136-a15f-a3d244922275",
   "metadata": {},
   "source": [
    "#### Rozdzielenie danych na zbiory treningowe oraz testowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b20a55a-639a-47e0-9ce1-ea71bd47ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65101b7d-edba-4e99-9369-686262e349fd",
   "metadata": {},
   "source": [
    "#### Wektoryzacja TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6133f437-2416-4740-9ffe-65da27cf82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "feature_names = tfidf.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4a60298-0896-4889-b26f-caba0258187b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wartość Score dla RandomForestCalssifier, przy TfidfVectorizer: 0.9757847533632287\n"
     ]
    }
   ],
   "source": [
    "clf_tfidf = RandomForestClassifier()\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "print(\"Wartość Score dla RandomForestCalssifier, przy TfidfVectorizer:\",clf_tfidf.score(X_test_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8237999c-e6e9-42fb-87a5-10023a8605ca",
   "metadata": {},
   "source": [
    "#### Wektoryzacja CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a8c9c41-d6a9-4b10-8a74-0dc78240e75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wartość Score dla RandomForestCalssifier, przy CountVectorizer: 0.9739910313901345\n"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer()\n",
    "X_train_count = count.fit_transform(X_train)\n",
    "X_test_count = count.transform(X_test)\n",
    "clf_count = RandomForestClassifier()\n",
    "clf_count.fit(X_train_count, y_train)\n",
    "print(\"Wartość Score dla RandomForestCalssifier, przy CountVectorizer:\",clf_count.score(X_test_count, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02682cbd-e1d7-4bd1-b07d-e0ceac11216f",
   "metadata": {},
   "source": [
    "#### Określenie które słowa są najwazniejsze oraz określenie ich wazności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84150d9b-1097-4c2b-b461-d655969a294c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Word  Word Importance\n",
      "1661     garbage         0.031545\n",
      "7093       32323         0.029733\n",
      "4593       board         0.025915\n",
      "2994        arul         0.021489\n",
      "1889       ardìä         0.019107\n",
      "...          ...              ...\n",
      "4850     busetop         0.000000\n",
      "4851    knowthis         0.000000\n",
      "4852  tirunelvai         0.000000\n",
      "4853     plumber         0.000000\n",
      "3910        jsut         0.000000\n",
      "\n",
      "[7820 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "importances = clf_tfidf.feature_importances_\n",
    "feature_imp_df = pd.DataFrame({'Word': feature_names, 'Word Importance': importances}).sort_values('Word Importance', ascending=False) \n",
    "print(feature_imp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab67421-1b8f-4d46-97d5-dda937e39331",
   "metadata": {},
   "source": [
    "#### Stworzenie listy z słów których wazność jest większa niż 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2b1e369-d376-4bcc-9354-2e3b84aa1408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['garbage',\n",
       " '32323',\n",
       " 'board',\n",
       " 'arul',\n",
       " 'ardìä',\n",
       " 'knickers',\n",
       " 'ukmobiledate',\n",
       " 'improve',\n",
       " 'distract',\n",
       " 'nyc',\n",
       " '3750',\n",
       " 'completed',\n",
       " 'carton',\n",
       " 'tape',\n",
       " 'later',\n",
       " 'sac',\n",
       " 'request',\n",
       " 'filthy',\n",
       " 'currently',\n",
       " 'treasure',\n",
       " 'loved',\n",
       " 'supose',\n",
       " 'hp',\n",
       " 'movie',\n",
       " 'league',\n",
       " 'havin',\n",
       " 'tncs',\n",
       " 'small',\n",
       " '7mahaveer',\n",
       " 'bitch',\n",
       " 'watching',\n",
       " 'youany',\n",
       " 'kintu',\n",
       " 'parish',\n",
       " 'ambitious',\n",
       " 'dying',\n",
       " 'jamz',\n",
       " 'topped',\n",
       " 'messageit',\n",
       " 'either',\n",
       " 'vodka',\n",
       " '89034',\n",
       " '500000',\n",
       " 'win',\n",
       " 'wwwapplausestorecom',\n",
       " 'send',\n",
       " 'cheetos',\n",
       " '3mobile',\n",
       " 'starving',\n",
       " 'castor',\n",
       " 'dodgey',\n",
       " 'lunchyou',\n",
       " 'oyea',\n",
       " 'somewhere',\n",
       " 'tmorrowpls',\n",
       " 'angel',\n",
       " 'considering',\n",
       " 'entropication',\n",
       " 'din',\n",
       " 'okie',\n",
       " 'course',\n",
       " 'jenny',\n",
       " 'favor',\n",
       " 'nokia6600',\n",
       " '140ppm',\n",
       " 'anywhere',\n",
       " 'complimentary',\n",
       " 'passion',\n",
       " 'kyou',\n",
       " 'av',\n",
       " 'deari',\n",
       " 'high',\n",
       " 'paining',\n",
       " 'updat',\n",
       " 'honey',\n",
       " 'rinu',\n",
       " 'ranju',\n",
       " 'religiously',\n",
       " 'scotland',\n",
       " 'babe',\n",
       " 'nag',\n",
       " 'max6month',\n",
       " 'ill',\n",
       " 'grave',\n",
       " 'blueu',\n",
       " 'fat',\n",
       " 'ive',\n",
       " 'gsex',\n",
       " 'leaf',\n",
       " 'drastic',\n",
       " 'm8s',\n",
       " 'careful',\n",
       " 'souveniers',\n",
       " 'sing',\n",
       " 'get',\n",
       " 'youd',\n",
       " 'exam',\n",
       " 'wordnot',\n",
       " 'mnths',\n",
       " 'lifeand',\n",
       " 'invited',\n",
       " 'shopwe',\n",
       " 'secretly',\n",
       " 'chex',\n",
       " 'sheffield',\n",
       " '06',\n",
       " 'teletext',\n",
       " 'wheres',\n",
       " 'argument',\n",
       " '29100',\n",
       " 'woodland',\n",
       " 'busy',\n",
       " 'cutefrnd',\n",
       " 'hustle',\n",
       " 'simple',\n",
       " 'gee',\n",
       " 'matched',\n",
       " 'wwwcnupdatescomnewsletter',\n",
       " 'kwish',\n",
       " '2naughty',\n",
       " 'second',\n",
       " 'treatin',\n",
       " 'wanting',\n",
       " 'lobby',\n",
       " 'httpwap',\n",
       " 'prakesh',\n",
       " 'crap',\n",
       " 'giving',\n",
       " 'greeni',\n",
       " 'msg',\n",
       " 'mmmmmmm',\n",
       " 'faithevening',\n",
       " 'case',\n",
       " 'ghodbandar',\n",
       " 'la3',\n",
       " 'oso',\n",
       " 'lor',\n",
       " 'double',\n",
       " 'nicenicehow',\n",
       " 'barry',\n",
       " 'otside',\n",
       " 'formal',\n",
       " '6th',\n",
       " '09058094507',\n",
       " 'cc',\n",
       " 'ecstasy',\n",
       " 'mrt',\n",
       " 'messenger',\n",
       " 'bstfrnd',\n",
       " 'soonlots',\n",
       " 'little',\n",
       " 'resub',\n",
       " 'erm',\n",
       " 'hellogorgeous']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_word_lis = feature_imp_df[feature_imp_df['Word Importance']>0.001].iloc[:,0].to_list()\n",
    "imp_word_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc5dbe6f-5739-4ad1-8588-492bc741a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_imp =  TfidfVectorizer(min_df = 0.001)\n",
    "X_train_imp = tfidf_imp.fit_transform(X_train)\n",
    "X_test_imp = tfidf_imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79a2d2fa-d565-4d15-93a9-4a17141b3570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "Best hyperparameter: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 150}\n",
      "Grid search for RandomForestClassifier Score Test (TfidfVectorizer): 0.9802690582959641\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier( n_jobs=-1)\n",
    "params_rf = {'max_depth': [ None, 15],\n",
    "             'min_samples_leaf': [1, 3],\n",
    "             'n_estimators': [ 50, 100, 150 ],\n",
    "             'min_samples_split': [2, 3, 5]}\n",
    "\n",
    "rf_gridsearch = GridSearchCV(random_forest,\n",
    "                             params_rf,\n",
    "                             scoring='f1_micro',\n",
    "                             cv=5,\n",
    "                             verbose=10, n_jobs=-1)\n",
    "rf_gridsearch.fit(X_train_imp, y_train)\n",
    "print('\\nBest hyperparameter:', rf_gridsearch.best_params_)\n",
    "model = rf_gridsearch.best_estimator_\n",
    "model.fit(X_train_imp, y_train)\n",
    "print(\"Grid search for RandomForestClassifier Score Test (TfidfVectorizer):\", model.score(X_test_imp, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "361103ad-8470-4fba-93dd-e166e549a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_imp =  CountVectorizer(min_df = 0.001)\n",
    "X_train_imp = tfidf_imp.fit_transform(X_train)\n",
    "X_test_imp = tfidf_imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aa3b938-2c15-4e4a-87ee-975c3d3f2aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "Best hyperparameter: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "Grid search for RandomForestClassifier Score Test (CountVectorizer): 0.979372197309417\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier( n_jobs=-1)\n",
    "params_rf = {'max_depth': [ None, 15 ],\n",
    "             'min_samples_leaf': [1, 3],\n",
    "             'n_estimators': [ 50, 100, 150 ],\n",
    "             'min_samples_split': [2, 3, 5]}\n",
    "\n",
    "rf_gridsearch = GridSearchCV(random_forest,\n",
    "                             params_rf,\n",
    "                             scoring='f1_micro',\n",
    "                             cv=5,\n",
    "                             verbose=10, n_jobs=-1)\n",
    "rf_gridsearch.fit(X_train_imp, y_train)\n",
    "print('\\nBest hyperparameter:', rf_gridsearch.best_params_)\n",
    "model = rf_gridsearch.best_estimator_\n",
    "model.fit(X_train_imp, y_train)\n",
    "print(\"Grid search for RandomForestClassifier Score Test (CountVectorizer):\", model.score(X_test_imp, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892411b0-4d90-4719-ad8e-ef1e7fa99354",
   "metadata": {},
   "source": [
    "#### Udało się poprwić wynik Score zarowno dla CountVectorizer jak i dla TfidfVectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
