{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a59826bf-a5a5-4233-a8f9-b1c4db7216ee",
   "metadata": {},
   "source": [
    "# 17.2 Zadanie Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b8d9c-0992-4c10-8d19-b9a4e0ecce6b",
   "metadata": {},
   "source": [
    "#### Import niezbednych bibliotek i paczek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e36c2c-92e1-495a-8037-a78d9fd1df6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\piotr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\piotr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\piotr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import itertools\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70aed96-f2d7-406e-a8d8-22eb620eb096",
   "metadata": {},
   "source": [
    "#### Definicje wszystkich funkcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f16a14a-e809-4c3c-818f-130a283dc62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_puncation(text):\n",
    "    cleaned = ''.join([word for word in text if word not in string.punctuation])\n",
    "    return cleaned\n",
    "def tokenize(text):\n",
    "    # Usunięcie wielkich liter\n",
    "    clean_text = text.lower()\n",
    "    # Tokenizacja\n",
    "    tokenized_text = nltk.word_tokenize(clean_text)\n",
    "    return tokenized_text\n",
    "def remove_stopwords(text):\n",
    "    without_stopwords = [word for word in text if word not in stopwords]\n",
    "    return without_stopwords\n",
    "def stemming(text):\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    return stemmed_words\n",
    "def lemmatizing(text):\n",
    "    lemmatized_words = [lemmater.lemmatize(word) for word in text]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d49463-97b5-4112-8a80-573296373501",
   "metadata": {},
   "source": [
    "#### Import pliku z danymi i jego formatowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "169c218d-ef76-44bf-bed1-f22d19d6b27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piotr\\AppData\\Local\\Temp\\ipykernel_31360\\1996902043.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  spam_dataset['Spam'] = spam_dataset['Spam'].replace(['ham', 'spam'], [0, 1])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Spam                                               Text\n",
       "0        0  Go until jurong point, crazy.. Available only ...\n",
       "1        0                      Ok lar... Joking wif u oni...\n",
       "2        1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3        0  U dun say so early hor... U c already then say...\n",
       "4        0  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567     1  This is the 2nd time we have tried 2 contact u...\n",
       "5568     0              Will Ì_ b going to esplanade fr home?\n",
       "5569     0  Pity, * was in mood for that. So...any other s...\n",
       "5570     0  The guy did some bitching but I acted like i'd...\n",
       "5571     0                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_dataset = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\", usecols=[0, 1], names=['Spam', 'Text'],\n",
    "                           skiprows=1)\n",
    "spam_dataset['Spam'] = spam_dataset['Spam'].replace(['ham', 'spam'], [0, 1])\n",
    "spam_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05288c2-f9c6-4320-961c-1cb3eb919af2",
   "metadata": {},
   "source": [
    "#### Obróbka danych - czyszczenie, tokenizacja, usuwanie stop words, Stemmed, Lametyzcja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f698ae3-42e2-4ccf-a164-d3ac44bc32e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam</th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "      <th>Tokenized_Text</th>\n",
       "      <th>WithoutStop_Text</th>\n",
       "      <th>Stemmed_Text</th>\n",
       "      <th>Lemmatized_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>Go until jurong point crazy Available only in ...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, gre...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, tho...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>[this, is, the, 2nd, time, we, have, tried, 2,...</td>\n",
       "      <td>[2nd, time, tried, 2, contact, u, u, å£750, po...</td>\n",
       "      <td>[2nd, time, tri, 2, contact, u, u, å£750, poun...</td>\n",
       "      <td>[2nd, time, tried, 2, contact, u, u, å£750, po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>Will Ì b going to esplanade fr home</td>\n",
       "      <td>[will, ì, b, going, to, esplanade, fr, home]</td>\n",
       "      <td>[ì, b, going, esplanade, fr, home]</td>\n",
       "      <td>[ì, b, go, esplanad, fr, home]</td>\n",
       "      <td>[ì, b, going, esplanade, fr, home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>Pity  was in mood for that Soany other suggest...</td>\n",
       "      <td>[pity, was, in, mood, for, that, soany, other,...</td>\n",
       "      <td>[pity, mood, soany, suggestions]</td>\n",
       "      <td>[piti, mood, soani, suggest]</td>\n",
       "      <td>[pity, mood, soany, suggestion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>The guy did some bitching but I acted like id ...</td>\n",
       "      <td>[the, guy, did, some, bitching, but, i, acted,...</td>\n",
       "      <td>[guy, bitching, acted, like, id, interested, b...</td>\n",
       "      <td>[guy, bitch, act, like, id, interest, buy, som...</td>\n",
       "      <td>[guy, bitching, acted, like, id, interested, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>Rofl Its true to its name</td>\n",
       "      <td>[rofl, its, true, to, its, name]</td>\n",
       "      <td>[rofl, true, name]</td>\n",
       "      <td>[rofl, true, name]</td>\n",
       "      <td>[rofl, true, name]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Spam                                               Text  \\\n",
       "0        0  Go until jurong point, crazy.. Available only ...   \n",
       "1        0                      Ok lar... Joking wif u oni...   \n",
       "2        1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        0  U dun say so early hor... U c already then say...   \n",
       "4        0  Nah I don't think he goes to usf, he lives aro...   \n",
       "...    ...                                                ...   \n",
       "5567     1  This is the 2nd time we have tried 2 contact u...   \n",
       "5568     0              Will Ì_ b going to esplanade fr home?   \n",
       "5569     0  Pity, * was in mood for that. So...any other s...   \n",
       "5570     0  The guy did some bitching but I acted like i'd...   \n",
       "5571     0                         Rofl. Its true to its name   \n",
       "\n",
       "                                           Cleaned_Text  \\\n",
       "0     Go until jurong point crazy Available only in ...   \n",
       "1                               Ok lar Joking wif u oni   \n",
       "2     Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3           U dun say so early hor U c already then say   \n",
       "4     Nah I dont think he goes to usf he lives aroun...   \n",
       "...                                                 ...   \n",
       "5567  This is the 2nd time we have tried 2 contact u...   \n",
       "5568                Will Ì b going to esplanade fr home   \n",
       "5569  Pity  was in mood for that Soany other suggest...   \n",
       "5570  The guy did some bitching but I acted like id ...   \n",
       "5571                          Rofl Its true to its name   \n",
       "\n",
       "                                         Tokenized_Text  \\\n",
       "0     [go, until, jurong, point, crazy, available, o...   \n",
       "1                        [ok, lar, joking, wif, u, oni]   \n",
       "2     [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3     [u, dun, say, so, early, hor, u, c, already, t...   \n",
       "4     [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "...                                                 ...   \n",
       "5567  [this, is, the, 2nd, time, we, have, tried, 2,...   \n",
       "5568       [will, ì, b, going, to, esplanade, fr, home]   \n",
       "5569  [pity, was, in, mood, for, that, soany, other,...   \n",
       "5570  [the, guy, did, some, bitching, but, i, acted,...   \n",
       "5571                   [rofl, its, true, to, its, name]   \n",
       "\n",
       "                                       WithoutStop_Text  \\\n",
       "0     [go, jurong, point, crazy, available, bugis, n...   \n",
       "1                        [ok, lar, joking, wif, u, oni]   \n",
       "2     [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3         [u, dun, say, early, hor, u, c, already, say]   \n",
       "4     [nah, dont, think, goes, usf, lives, around, t...   \n",
       "...                                                 ...   \n",
       "5567  [2nd, time, tried, 2, contact, u, u, å£750, po...   \n",
       "5568                 [ì, b, going, esplanade, fr, home]   \n",
       "5569                   [pity, mood, soany, suggestions]   \n",
       "5570  [guy, bitching, acted, like, id, interested, b...   \n",
       "5571                                 [rofl, true, name]   \n",
       "\n",
       "                                           Stemmed_Text  \\\n",
       "0     [go, jurong, point, crazi, avail, bugi, n, gre...   \n",
       "1                          [ok, lar, joke, wif, u, oni]   \n",
       "2     [free, entri, 2, wkli, comp, win, fa, cup, fin...   \n",
       "3         [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
       "4     [nah, dont, think, goe, usf, live, around, tho...   \n",
       "...                                                 ...   \n",
       "5567  [2nd, time, tri, 2, contact, u, u, å£750, poun...   \n",
       "5568                     [ì, b, go, esplanad, fr, home]   \n",
       "5569                       [piti, mood, soani, suggest]   \n",
       "5570  [guy, bitch, act, like, id, interest, buy, som...   \n",
       "5571                                 [rofl, true, name]   \n",
       "\n",
       "                                        Lemmatized_Text  \n",
       "0     [go, jurong, point, crazy, available, bugis, n...  \n",
       "1                        [ok, lar, joking, wif, u, oni]  \n",
       "2     [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "3         [u, dun, say, early, hor, u, c, already, say]  \n",
       "4     [nah, dont, think, go, usf, life, around, though]  \n",
       "...                                                 ...  \n",
       "5567  [2nd, time, tried, 2, contact, u, u, å£750, po...  \n",
       "5568                 [ì, b, going, esplanade, fr, home]  \n",
       "5569                    [pity, mood, soany, suggestion]  \n",
       "5570  [guy, bitching, acted, like, id, interested, b...  \n",
       "5571                                 [rofl, true, name]  \n",
       "\n",
       "[5572 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_dataset['Cleaned_Text'] = spam_dataset['Text'].apply(lambda x: remove_puncation(x))\n",
    "spam_dataset['Tokenized_Text'] = spam_dataset['Cleaned_Text'].apply(lambda x: tokenize(x))\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "spam_dataset['WithoutStop_Text'] = spam_dataset['Tokenized_Text'].apply(lambda x: remove_stopwords(x))\n",
    "stemmer = nltk.PorterStemmer()\n",
    "spam_dataset['Stemmed_Text'] = spam_dataset['WithoutStop_Text'].apply(lambda x: stemming(x))\n",
    "lemmater = nltk.WordNetLemmatizer()\n",
    "spam_dataset['Lemmatized_Text'] = spam_dataset['WithoutStop_Text'].apply(lambda x: lemmatizing(x))\n",
    "spam_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458d99e-b0de-437a-b42c-a6414ae9b14d",
   "metadata": {},
   "source": [
    "#### Wektoryzacja tekstu i rozdzielenie na X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c275ce57-6a22-493c-9fbb-f4ad6a90c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(spam_dataset['Lemmatized_Text'].apply(lambda x: ' '.join(x)))\n",
    "y = spam_dataset['Spam']\n",
    "feature_names = tfidf.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12280cfa-33f7-4136-a15f-a3d244922275",
   "metadata": {},
   "source": [
    "#### Rozdzielenie danych na zbiory treningowe oraz testowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b20a55a-639a-47e0-9ce1-ea71bd47ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4a60298-0896-4889-b26f-caba0258187b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8663677130044843"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02682cbd-e1d7-4bd1-b07d-e0ceac11216f",
   "metadata": {},
   "source": [
    "#### Określenie które słowa są najwazniejsze oraz określenie ich wazności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84150d9b-1097-4c2b-b461-d655969a294c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Word  Word Importance\n",
      "7618         pimple         0.037414\n",
      "5193         mirror         0.034702\n",
      "2453          weird         0.033022\n",
      "7368        trishul         0.029568\n",
      "7679        hostile         0.026637\n",
      "...             ...              ...\n",
      "2980         marley         0.000000\n",
      "2979  stereophonics         0.000000\n",
      "2978        lastest         0.000000\n",
      "2977        charles         0.000000\n",
      "8842       bitching         0.000000\n",
      "\n",
      "[8843 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "importances = clf.feature_importances_\n",
    "feature_imp_df = pd.DataFrame({'Word': feature_names, 'Word Importance': importances}).sort_values('Word Importance', ascending=False) \n",
    "print(feature_imp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab67421-1b8f-4d46-97d5-dda937e39331",
   "metadata": {},
   "source": [
    "#### Stworzenie listy z złów których wazność jest większa niż 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2b1e369-d376-4bcc-9354-2e3b84aa1408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pimple',\n",
       " 'mirror',\n",
       " 'weird',\n",
       " 'trishul',\n",
       " 'hostile',\n",
       " 'mah',\n",
       " 'wood',\n",
       " 'canal',\n",
       " 'belovd',\n",
       " 'nvq',\n",
       " 'incorrect',\n",
       " 'videopic',\n",
       " 'ringtone',\n",
       " 'shopping',\n",
       " '2814032',\n",
       " 'jstfrnd',\n",
       " 'keyword',\n",
       " 'versus',\n",
       " '08718738002',\n",
       " 'sp',\n",
       " 'meive',\n",
       " '25p',\n",
       " 'frnt',\n",
       " '5p',\n",
       " 'basket',\n",
       " 'twat',\n",
       " 'prescription',\n",
       " 'bt',\n",
       " 'compensation',\n",
       " 'bbdpooja',\n",
       " '600',\n",
       " '09050280520',\n",
       " 'cnn',\n",
       " 'starwars3',\n",
       " '9yt',\n",
       " 'enjoy',\n",
       " 'loud',\n",
       " 'check',\n",
       " 'adult',\n",
       " 'hard',\n",
       " '830',\n",
       " 'srsly',\n",
       " 'texas',\n",
       " 'direct',\n",
       " 'ringtonefrom',\n",
       " 'cutefrnd',\n",
       " 'blanket',\n",
       " 'matra',\n",
       " 'startedindia',\n",
       " 'fair',\n",
       " 'knackered',\n",
       " 'dent',\n",
       " 'warned',\n",
       " 'b4190604',\n",
       " 'real',\n",
       " 'horse',\n",
       " '1yf',\n",
       " 'oredi',\n",
       " 'tension',\n",
       " 'jess',\n",
       " 'arsenal',\n",
       " 'owed',\n",
       " 'manual',\n",
       " 'bedroom',\n",
       " 'october',\n",
       " 'daddy',\n",
       " 'possiblehope',\n",
       " '2untamed',\n",
       " '3wife',\n",
       " '8am',\n",
       " 'jesus',\n",
       " 'tomo',\n",
       " 'worth',\n",
       " 'skirt',\n",
       " 'ups',\n",
       " 'alexs',\n",
       " 'local',\n",
       " 'learn',\n",
       " 'girld',\n",
       " 'lip',\n",
       " 'studying',\n",
       " 'wwwsantacallingcom',\n",
       " 'ceiling',\n",
       " 'morning',\n",
       " 'hasbroin',\n",
       " 'hi',\n",
       " 'oooooh',\n",
       " 'wendy',\n",
       " 'franxx',\n",
       " 'copy',\n",
       " 'machiany',\n",
       " 'transfered',\n",
       " 'autocorrect',\n",
       " 'chop',\n",
       " 'plaza',\n",
       " '220cm2',\n",
       " 'allah',\n",
       " 'could',\n",
       " 'channel',\n",
       " '88066',\n",
       " 'helping',\n",
       " 'stage',\n",
       " 'ranju',\n",
       " 'threw',\n",
       " 'life',\n",
       " 'stocked',\n",
       " 'loo',\n",
       " 'pile',\n",
       " 'paragon',\n",
       " 'wrking',\n",
       " 'allows',\n",
       " 'reppurcussions',\n",
       " 'passthey',\n",
       " 'tlk',\n",
       " 'nonetheless',\n",
       " 'golden',\n",
       " 'reslove',\n",
       " 'anthony',\n",
       " 'natural',\n",
       " 'alex',\n",
       " 'provided',\n",
       " 'rajipls',\n",
       " 'amused']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_word_lis = feature_imp_df[feature_imp_df['Word Importance']>0.001].iloc[:,0].to_list()\n",
    "imp_word_lis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ec543-1d19-44a3-b6f1-a4bb7a8dc6d8",
   "metadata": {},
   "source": [
    "#### Tworzymu nową kolumnę w spam_dataset gdzie będziemy mieć wyłącznie wazne słowa po lametyzacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e93c5928-4f6d-4684-8a17-dfa69e3118df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_only(text):\n",
    "    important_only = [word for word in text if word in imp_word_lis]\n",
    "    return important_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b15fe26-b8ad-4f22-b3dc-7435d26c33d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           []\n",
       "1           []\n",
       "2           []\n",
       "3           []\n",
       "4       [life]\n",
       "         ...  \n",
       "5567        []\n",
       "5568        []\n",
       "5569        []\n",
       "5570        []\n",
       "5571        []\n",
       "Name: Important_Text, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_dataset['Important_Text'] = spam_dataset['Lemmatized_Text'].apply(lambda x: important_only(x))\n",
    "spam_dataset['Important_Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b512d445-2617-4491-ba6e-88dd0036b886",
   "metadata": {},
   "source": [
    "#### Przeprowadzamy wektoryzacje teksty którego ważnść jest większa niż 0.001, rozdzielamy dane "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2728fb18-19a5-49f4-a1d6-91c618e946b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_imp =  TfidfVectorizer()\n",
    "X = tfidf_imp.fit_transform(spam_dataset['Important_Text'].apply(lambda x: ' '.join(x)))\n",
    "y = spam_dataset['Spam']\n",
    "X_train_imp, X_test_imp, y_train_imp, y_test_imp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f141c61-00da-4140-9654-0ca527868f5b",
   "metadata": {},
   "source": [
    "#### Tworzymy GridSearch dla najwazniejszych parametrów klasyfikatora, szkolimy go z najlepszymi parametrami i odczytujemy wynik. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d169319-921b-42ad-9e84-c4e4eccba8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "\n",
      "Best hyperparameter: {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 40}\n",
      "LogisticRegression F1_score Train: 0.8759\n",
      "LogisticRegression F1_score Test: 0.8762\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier( n_jobs=-1)\n",
    "params_rf = {'max_depth': [ 15, 20, 25],\n",
    "             'min_samples_leaf': [2, 3, 5],\n",
    "             'n_estimators': [  25, 30, 40 ],\n",
    "             'min_samples_split': [2, 3, 5]}\n",
    "\n",
    "rf_gridsearch = GridSearchCV(random_forest,\n",
    "                             params_rf,\n",
    "                             scoring='f1_micro',\n",
    "                             cv=5,\n",
    "                             verbose=10, n_jobs=-1)\n",
    "rf_gridsearch.fit(X_train_imp, y_train_imp)\n",
    "print('\\nBest hyperparameter:', rf_gridsearch.best_params_)\n",
    "model = rf_gridsearch.best_estimator_\n",
    "model.fit(X_train_imp, y_train_imp)\n",
    "predictions_test = model.predict(X_test_imp)\n",
    "predictions_train = model.predict(X_train_imp)\n",
    "f1_score_train = f1_score(y_train_imp, predictions_train, average='micro').round(4)\n",
    "f1_score_test = f1_score(y_test_imp, predictions_test, average='micro').round(4)\n",
    "print(\"LogisticRegression F1_score Train:\", f1_score_train)\n",
    "print(\"LogisticRegression F1_score Test:\", f1_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892411b0-4d90-4719-ad8e-ef1e7fa99354",
   "metadata": {},
   "source": [
    "#### Udało się poprwić wynik F1 Score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
